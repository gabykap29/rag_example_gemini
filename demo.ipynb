{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad187ee9",
   "metadata": {},
   "source": [
    "# RAG LANGCHAIN + Gemini + Nomic + CHROMADB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe97b83e",
   "metadata": {},
   "source": [
    "Para este proyecto, se necesita las siguientes dependencias: \n",
    "* Streamlit\n",
    "* Langchain_community\n",
    "* Lanchain_core\n",
    "* langchain_ollama (Embeddings)\n",
    "* langchain-google-genai (Gemini)\n",
    "* python-dotenv\n",
    "\n",
    "\n",
    "pip install protobuf==4.25.3 grpcio==1.60.0 langchain langchain-community langchain-core langchain-google-genai langchain-ollama google-generativeai streamlit chromadb \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37644424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482aa842",
   "metadata": {},
   "source": [
    "### Custom prompts \n",
    "Prompt que sera enviada en cada peticion hacia el modelo, por lo tanto, no debe ser tan larga, debe ser concisa y con el suficiente contexto para trabajar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef6060",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_template = \"\"\"\n",
    "Actúa como asistente educativo que genera preguntas de opción múltiple adaptativas.\n",
    "\n",
    "INPUTS:\n",
    "- Materia\n",
    "- Unidad Temática\n",
    "- Evidencia: Conocimiento | Procedimiento | Producto\n",
    "- Nivel: 1=Basico-Bajo | 2=Basico | 3=Satisfactorio | 4=Avanzado\n",
    "\n",
    "OUTPUT:\n",
    "- SOLO JSON válido con esta estructura:\n",
    "\n",
    "{{\n",
    "  \"Titulo\": \"string ≤80\",\n",
    "  \"Consigna\": \"pregunta clara\",\n",
    "  \"Contexto\": \"string ≤200\",\n",
    "  \"Dificultad\": \"Basico-Bajo\" | \"Basico\" | \"Medio\" | \"Alto\",\n",
    "  \"TiempoEstimado\": \"MM:SS (01:00-02:00)\",\n",
    "  \"VectorNivelOpciones\": {{\n",
    "    \"OpcionA\": [\"Bajo\", \"Medio\", \"Alto\", \"Alto\"],\n",
    "    \"OpcionB\": [\"Medio\", \"Medio\", \"Bajo\", \"Alto\"],\n",
    "    \"OpcionC\": [\"Alto\", \"Bajo\", \"Medio\", \"Medio\"],\n",
    "    \"OpcionD\": [\"Medio\", \"Alto\", \"Bajo\", \"Bajo\"]\n",
    "  }},\n",
    "  \"Opciones\": {{ \"A\": \"str\", \"B\": \"str\", \"C\": \"str\", \"D\": \"str\" }},\n",
    "  \"RespuestaCorrecta\": \"A\"|\"B\"|\"C\"|\"D\"\n",
    "}}\n",
    "\n",
    "REGLAS:\n",
    "- Una única respuesta correcta, 3 distractores plausibles.\n",
    "- Lenguaje acorde al nivel.\n",
    "- Consigna = pregunta directa alineada con la evidencia.\n",
    "- Contexto breve y realista.\n",
    "- Opciones similares en longitud y sin pistas.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4988a71",
   "metadata": {},
   "source": [
    "### Directorio de PDFs ya utilizados para el embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf05441",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_directory = \"./data\"\n",
    "db_directory = \"./db\"\n",
    "\n",
    "if not os.path.exists(db_directory):\n",
    "    os.makedirs(db_directory)\n",
    "if not os.path.exists(pdf_directory):\n",
    "    os.makedirs(pdf_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30c07d",
   "metadata": {},
   "source": [
    "### Configuración del Modelo de Embeddings.\n",
    "Esta es la configuración para aplicar el modelo de embeddings con ollama.\n",
    "El modelo a utilizar es ***nomic-embed-text.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c634b5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_37636\\3662783597.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_vector_stores(materia):\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    client = chromadb.PersistentClient(path=db_directory)\n",
    "    return Chroma(\n",
    "        client=client,\n",
    "        collection_name=materia.lower().replace(\" \", \"_\"),\n",
    "        embedding_function=embeddings,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84be374",
   "metadata": {},
   "source": [
    "### Procesamiento de los PDFs para embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dedc8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def upload_pdf(file):\n",
    "    with open(pdf_directory + file.name, \"wb\") as f:\n",
    "        f.write(file.getbuffer())\n",
    "        \n",
    "def load_pdf(file):\n",
    "    loader = PDFPlumberLoader(file)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "def text_splitter(documents, course_name):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "    )    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    if course_name:\n",
    "        for _, doc in enumerate(chunks):\n",
    "            doc.metadata[\"course_name\"] = course_name\n",
    "    return chunks\n",
    "\n",
    "def index_docs(documents, materia):\n",
    "    vectorstore = get_vector_stores(materia)\n",
    "    vectorstore.add_documents(documents)\n",
    "    vectorstore.persist()\n",
    "    print(\"Documents indexed successfully. Numbers of documents:\", len(documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8369e",
   "metadata": {},
   "source": [
    "### Retrieve docs\n",
    "Recuperación los documentos por busqueda de similitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf494a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query, coleccion):\n",
    "    vectorstore = get_vector_stores(coleccion)\n",
    "    docs = vectorstore.similarity_search(query, k=5)\n",
    "    print(\"Retrieved documents:\", len(docs))\n",
    "    if docs:\n",
    "        return docs\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083a24f",
   "metadata": {},
   "source": [
    "### Funcion para obtener el hash del documento\n",
    "\n",
    "Esta funcion sirve para corroborar de que no fue vectorizado aun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hash(file_path):\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        buf = f.read()\n",
    "        hasher.update(buf)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def is_pdf_already_indexed(file_path, materia):\n",
    "    vectorstore = get_vector_stores(materia)\n",
    "    result = vectorstore.similarity_search(file_path, k=1)\n",
    "    if result:\n",
    "        for doc in result:\n",
    "            if doc.metadata.get(\"file_hash\") == get_file_hash(file_path):\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c559cb8",
   "metadata": {},
   "source": [
    "### Función para autentificar el modelo. \n",
    "En este caso, se utiliza genai, la libreria de google para comunicarse con Gemini Ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a2c1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    st.error(\"API key not found. Please set the API_KEY environment variable.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df4296c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9767a365",
   "metadata": {},
   "source": [
    "### Funcion para obtener la respuesta de Gemini en formato Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415737c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\Desktop\\rag\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-18 16:07:12.251 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-18 16:07:12.253 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-18 16:07:12.254 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-18 16:07:12.258 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-18 16:07:12.821 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\gabri\\Desktop\\rag\\env\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-08-18 16:07:12.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-18 16:07:12.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-18 16:07:12.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-18 16:07:12.824 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-18 16:07:12.825 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-18 16:07:12.827 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-18 16:07:12.828 Session state does not function when running a script without `streamlit run`\n",
      "2025-08-18 16:07:12.829 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-18 16:07:12.830 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-08-18 16:07:12.831 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def generate_response_stream(context, materia, unidad_tematica, evidencia, nivel):\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        temperature=0.2,\n",
    "        max_tokens=2000,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", custom_template),\n",
    "        (\"user\", \"Materia: {materia}, Unidad_Tematica: {unidad_tematica}, Evidencia: {evidencia}, Nivel: {nivel}, Contexto: {context}\"),\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    \n",
    "    input_dict = {\n",
    "        \"materia\": materia,\n",
    "        \"unidad_tematica\": unidad_tematica,\n",
    "        \"evidencia\": evidencia,\n",
    "        \"nivel\": nivel,\n",
    "        \"context\": context\n",
    "    }\n",
    "    print(\"Input dictionary:\", input_dict)\n",
    "    \n",
    "    for chunk in chain.stream(input_dict):\n",
    "        yield chunk \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa6fb08",
   "metadata": {},
   "source": [
    "### Visualizar la interfaz con STREAMLIT para demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b186c7e9",
   "metadata": {},
   "source": [
    "### Inputs para el embeddings\n",
    "* Coleccion\n",
    "* El documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f9cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 16:19:50.657 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 16:19:50.660 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 16:19:50.661 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 16:19:50.671 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 16:19:50.769 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\gabri\\Desktop\\RAG - Gemini\\env\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-05-13 16:19:50.771 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 16:19:50.772 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 16:19:50.774 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 16:19:50.776 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 16:19:50.777 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 16:19:50.778 Session state does not function when running a script without `streamlit run`\n",
      "2025-05-13 16:19:50.779 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-13 16:19:50.780 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "uploaded_file = st.file_uploader(\"Sube un archivo PDF\", type=\"pdf\")\n",
    "coleccion = st.text_input(\"Coleccion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaec1d6",
   "metadata": {},
   "source": [
    "### Realizar el embeddings\n",
    "Cuando un pdf es subido, se realiza el procesamiento y vectorizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abf7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if uploaded_file and coleccion:\n",
    "    upload_pdf(uploaded_file)\n",
    "    documents = load_pdf(pdf_directory + uploaded_file.name)\n",
    "\n",
    "    file_hash = get_file_hash(pdf_directory + uploaded_file.name)\n",
    "    if is_pdf_already_indexed(file_hash, coleccion):\n",
    "        st.warning(\"Este PDF ya ha sido indexado.\")\n",
    "    else:\n",
    "        chunked_documents = text_splitter(documents)\n",
    "        for doc in chunked_documents:\n",
    "            doc.metadata[\"file_hash\"] = file_hash\n",
    "            doc.metadata[\"course_name\"] = coleccion.lower().replace(\" \", \"_\")\n",
    "\n",
    "            print(\"--- Documento a indexar ---\")\n",
    "            print(doc.page_content)\n",
    "            print(doc.metadata)\n",
    "        index_docs(chunked_documents)\n",
    "        st.success(\"PDF subido y procesado correctamente.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0288d0",
   "metadata": {},
   "source": [
    "### Inputs para el instructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12acf183",
   "metadata": {},
   "outputs": [],
   "source": [
    "materia = st.text_input(\"Materia\", key=\"materia_generacion\")\n",
    "unidad_tematica = st.text_input(\"Unidad Temática\", key=\"unidad\")\n",
    "evidencia = st.text_area(\"Evidencia\", key=\"evidencia\")\n",
    "nivel = st.number_input(\"Nivel del Estudiante\", min_value=1, max_value=4, step=1, key=\"nivel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c14f474",
   "metadata": {},
   "source": [
    "### Petición al modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbca7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if st.button(\"Generar Actividades\"):\n",
    "    if materia and unidad_tematica and evidencia:\n",
    "        st.info(f\"🎯 Generando actividades para {materia}, unidad: {unidad_tematica}, nivel: {nivel}\")\n",
    "    else:\n",
    "        st.warning(\"⚠️ Completa todos los campos antes de generar.\")\n",
    "  \n",
    "    related_documents = retrieve_docs(unidad_tematica, materia)\n",
    "\n",
    "\n",
    "    contexto = \"\\n\".join(doc.page_content for doc in related_documents) if related_documents else \"\"\n",
    "\n",
    "    message_placeholder = st.chat_message(\"assistant\").empty()\n",
    "    full_response = \"\"\n",
    "\n",
    "    for chunk in generate_response_stream(contexto, materia, unidad_tematica, evidencia, nivel):\n",
    "\n",
    "        full_response += chunk  # cada chunk trae parte del texto\n",
    "        message_placeholder.markdown(full_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
